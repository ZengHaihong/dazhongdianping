{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from IPpool.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import os.path\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException,WebDriverException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from requests.exceptions import ProxyError,ConnectionError,Timeout,HTTPError,ChunkedEncodingError\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import Ipynb_importer\n",
    "from IPpool import GetHeaders\n",
    "from IPpool import Ip_pool\n",
    "import random\n",
    "import copy\n",
    "# import requests_toolbelt.adapters.appengine\n",
    "\n",
    "# requests_toolbelt.adapters.appengine.monkeypatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**************将装机地址变成一组一组*****************\n",
    "def get_chunk(adr_ls,n):\n",
    "    new_adr_ls = copy.deepcopy(adr_ls)\n",
    "    adr_chunk = [new_adr_ls[i:i + n] for i in range(0, len(new_adr_ls), n)]\n",
    "    return adr_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#设置谷歌浏览器的参数\n",
    "def set_chrome_param():\n",
    "    #PROXY = '175.155.24.61:808'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    chrome_path = '/home/zhh/下载/chromedriver'  \n",
    "    #options.add_argument('--proxy-server={0}'.format(PROXY))\n",
    "    # 设置成中文\n",
    "    options.add_argument('lang=zh_CN.UTF-8')\n",
    "    # 添加头部\n",
    "    #1允许所有图片；2阻止所有图片；3阻止第三方服务器图片\n",
    "    prefs = {'profile.default_content_setting_values': { 'images':2}}\n",
    "    options.add_experimental_option('prefs', prefs)\n",
    "    options.add_argument('user-agent=\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.103 Safari/537.36\"')\n",
    "    chrome = webdriver.Chrome(chrome_path,chrome_options=options)\n",
    "    return chrome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#requests请求网页，返回一个BeautifulSoup对象\n",
    "def requests_visit_url(url,prox_ip=None):\n",
    "    headers = {\"Accept\":\"application/json, text/javascript, */*; q=0.01\",\n",
    "                    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "                    \"Accept-Language\":\"zh-CN,zh;q=0.8,en;q=0.6\",\n",
    "                    \"Connection\":\"keep-alive\",\n",
    "                    \"Cookie\":\"_hc.v=f72dbe69-bcfc-a9d4-1f7f-d8daeca1ad7f.1505634421; __utma=1.1801105885.1505634423.1505634423.1505636820.2; __utmz=1.1505634423.1.1.utmcsr=google.com.hk|utmccn=(referral)|utmcmd=referral|utmcct=/; _lxsdk_cuid=15e956d9d5ec8-0a28c942cf39be-2a044871-100200-15e956d9d5fc8; _lxsdk=15e956d9d5ec8-0a28c942cf39be-2a044871-100200-15e956d9d5fc8; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; __mta=222713764.1505836341840.1507819135553.1507819166825.4; JSESSIONID=D2D3D245867334198A59E676F146C5A8; aburl=1; cy=4; cye=guangzhou; s_ViewType=10; _lxsdk_s=15f2012787a-db4-26a-c8%7C%7C47\",\n",
    "                    \"Host\":\"www.dianping.com\",\n",
    "                    \"Referer\":\"http://www.dianping.com/shop/92374637\",\n",
    "                    #\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36\",\n",
    "                    \"X-Requested-With\":\"XMLHttpRequest\"}\n",
    "    headers[\"User-Agent\"] = random.choice(GetHeaders().user_agent_list)\n",
    "    #headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.103 Safari/537.36\"}\n",
    "    is_continue = True\n",
    "    num = 0\n",
    "    while is_continue:\n",
    "        num +=1\n",
    "        print(\"访问{0}次\".format(num))\n",
    "        try:\n",
    "            if prox_ip is None:\n",
    "                res = requests.get(url,headers = headers)  #访问url，不设代理ip访问\n",
    "            else:\n",
    "                http = prox_ip[0]\n",
    "                ip = http+\"://\"+prox_ip[1]\n",
    "                proxies={http:ip}\n",
    "                res = requests.get(url,headers = headers,proxies = proxies) #设代理ip访问\n",
    "            html =  res.content.decode('utf-8')\n",
    "            soup  =  BeautifulSoup(html,'lxml')\n",
    "            is_continue = False \n",
    "            \n",
    "        except HTTPError as e:\n",
    "            print(\"断网\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Timeout as e:\n",
    "            print(\"请求超时\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except ConnectionError as e:\n",
    "            print(\"访问被拒\")\n",
    "            is_continue = True\n",
    "            time.sleep(2)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#缓存结果\n",
    "def save_cache(result,filename):\n",
    "    with open(filename,'wb') as f1:\n",
    "        pickle.dump(result,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载缓存\n",
    "def read_cache(filename):\n",
    "    with open(filename,'rb')as f1:\n",
    "        result = pickle.load(f1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 切换路径\n",
    "def get_spc_path(father_path,child_path):\n",
    "    os.chdir(father_path)\n",
    "    if os.path.exists(child_path):\n",
    "        os.chdir(child_path)\n",
    "    else:\n",
    "        os.mkdir(child_path)\n",
    "        os.chdir(child_path)\n",
    "    print(\"done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.二级分类层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#获取所有的二级分类url\n",
    "def classify(city_url,city,big_class):\n",
    "    soup = requests_visit_url(city_url)\n",
    "    clf_ls = []\n",
    "    for i in soup.select(\"#classfy a\"):\n",
    "        clf_url = i[\"href\"] #分类网址\n",
    "        clf_tl = i.text.strip() # 分类标题\n",
    "        clf_ls.append([big_class,city,clf_tl,clf_url])\n",
    "    return clf_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# clf_ls = classify(\"http://www.dianping.com/search/category/7/10\",'深圳',\"美食\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.区县层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_region(class_ls=None,prox_ip=None):\n",
    "    print('正在爬取{0}'.format(class_ls[-2]))\n",
    "    url = class_ls[-1]  #url\n",
    "    soup = requests_visit_url(url,prox_ip) #访问\n",
    "    region_ls = [] #地区的列表\n",
    "    for i in soup.select(\"#region-nav a\"):\n",
    "        if i.text.strip() != \"不限\":\n",
    "            region_url = i[\"href\"] \n",
    "            region_tl = i.text.strip()\n",
    "            region = class_ls[:-1]\n",
    "            region.extend([region_tl,region_url])\n",
    "            region_ls.append(region)\n",
    "    return region_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ip = Ip_pool(['http://www.xicidaili.com/nn/'])\n",
    "# ip_pool = ip.get_ip_pool(test_url='http://www.dianping.com/search/category/7/10/g104',timeout =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# prox_ip = random.choice(ip_pool)\n",
    "# region_ls = []\n",
    "# for i in clf_ls:\n",
    "#     print(i)\n",
    "#     sub_region = get_region(i,prox_ip)\n",
    "#     region_ls.append(sub_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# region_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.商圈层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_circle(region_ls=None):\n",
    "    url = region_ls[-1]   #url网址\n",
    "    soup = requests_visit_url(url) #访问BeautifulSoup\n",
    "    circle_ls =[]             \n",
    "    for i in soup.select(\"#region-nav-sub a\"):  #商圈信息\n",
    "        if i.span.text.strip()!=\"不限\":   #剔除掉不限的url\n",
    "            circle_url = i[\"href\"]       #商圈url\n",
    "            circle_tit = i.span.text.strip()  #商圈名\n",
    "            circle = region_ls[:-1] \n",
    "            circle.extend([circle_tit,circle_url]) \n",
    "            circle_ls.append(circle)\n",
    "            \n",
    "    #如果没有商圈的话，就拿原来的url当作是商圈的url\n",
    "    if circle_ls==[]:\n",
    "        print(region_ls[-2]+\"没有子分类！！\")\n",
    "        region_ls.insert(4,\"没有商圈\")\n",
    "        circle_ls.append(region_ls)\n",
    "\n",
    "    return circle_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# circle_ls = get_circle(['美食',\n",
    "#   '广州',\n",
    "#   '茶餐厅',\n",
    "#   '天河区',\n",
    "#   'http://www.dianping.com/search/category/4/10/g207r22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.商家信息层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取每个商家的链接、店名\n",
    "def get_stores(circle_ls = None):\n",
    "    store_ls=[] #存储的列表\n",
    "    url = circle_ls[-1]  #url\n",
    "    soup = requests_visit_url_test(url,is_use_ip=True,timeout=2)  #访问当前主页\n",
    "    is_continue = True\n",
    "    page = 0\n",
    "    while is_continue:\n",
    "        page += 1\n",
    "        print(\"正在爬取第{0}页\".format(page))\n",
    "        \n",
    "        #解析数据\n",
    "        if soup.select(\".content\") ==[]:\n",
    "            print(\"此页没有任何信息！！\")\n",
    "            store_ls = []\n",
    "            is_continue = False \n",
    "            \n",
    "        else:\n",
    "            for i in soup.select(\".content\"):  \n",
    "                for j in i.select(\".pic\"):\n",
    "                    store_name = j.img[\"alt\"]   #店名\n",
    "                    link = j.a[\"href\"]     #详情层\n",
    "                    store = circle_ls[:5]  #只取前5位\n",
    "                    store.extend([url,store_name,link]) \n",
    "                    store_ls.append(store)\n",
    "\n",
    "\n",
    "            #获取下一页的数据\n",
    "            time.sleep(2)\n",
    "\n",
    "            next_tag = soup.find_all(\"a\",text = re.compile(\"下一页\"))  #获取字符属性为下一页的url\n",
    "            if next_tag != []:                                         #保证含有字符属性\n",
    "                url = next_tag[0][\"href\"]                              #获取下一页的url\n",
    "                soup = requests_visit_url(url)                         #更新BeautifulSoup对象\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                url = ''\n",
    "                is_continue  = False                                  #如果没有下一页，就停止更新\n",
    "    \n",
    "    return store_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# store_ls = get_stores(['美食',\n",
    "#   '广州',\n",
    "#   '茶餐厅',\n",
    "#   '天河区',\n",
    "#   '天河城/体育中心',\n",
    "#   'http://www.dianping.com/search/category/4/10/g207r1519'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### get_info_vesion2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.详情层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获取详情层的数据，如星级用户、浏览人数、均价\n",
    "def get_info(store_ls=None):\n",
    "    \n",
    "    #首先先保证导入的数据不为空\n",
    "    if store_ls !=[]:\n",
    "        url = store_ls[-1] #详情信息表\n",
    "        soup = requests_visit_url(url)\n",
    "        try:\n",
    "            start = soup.select(\".mid-rank-stars\")[0][\"title\"]       #星级用户\n",
    "        except Exception as e:\n",
    "            print(\"start\",e)\n",
    "            start = ''\n",
    "\n",
    "        try:\n",
    "            review_count = soup.select(\"#reviewCount\")[0].text  #浏览人数\n",
    "        except Exception as e:\n",
    "            review_count = ''\n",
    "            print(\"review_coun\",e)\n",
    "\n",
    "        try:\n",
    "            avgprice = soup.select(\"#avgPriceTitle\")[0].text   #均价\n",
    "        except Exception as e:\n",
    "            avgprice = ''\n",
    "            print(\"avgprice\",e)\n",
    "\n",
    "        #评分系统\n",
    "        try:\n",
    "            score= []\n",
    "            for j in soup.select(\"#comment_score .item\"):\n",
    "                score.append(j.text)\n",
    "            score = \"|\".join(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            score = \"\"\n",
    "            print(\"score\",e)\n",
    "\n",
    "        try:   \n",
    "            address = soup.select(\".address .item\")[0].text.strip() #地址\n",
    "        except Exception as e:\n",
    "            address = ''\n",
    "            print(\"address出错了\",e)\n",
    "\n",
    "        #电话号码\n",
    "        try:\n",
    "            tel =[]\n",
    "            for i in soup.select(\".tel .item\"):\n",
    "                tel.append(i.text)\n",
    "            tel = \"|\".join(tel)\n",
    "\n",
    "        except:\n",
    "            tel=\"\"\n",
    "            print(\"tel出错了\",e)\n",
    "\n",
    "        #路径\n",
    "        try:\n",
    "            path = []\n",
    "            for i in soup.select(\".breadcrumb a\"):\n",
    "                path.append(i.text.strip())\n",
    "            path ='>'.join(path)\n",
    "        except Exception as e:\n",
    "            print(\"path出错了\",e)\n",
    "            path =''\n",
    "\n",
    "        info = store_ls[:]\n",
    "        info.extend([start,review_count,avgprice,score,address,tel,path])\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_info(['美食',\n",
    "#   '广州',\n",
    "#   '茶餐厅',\n",
    "#   '天河区',\n",
    "#   '天河城/体育中心',\n",
    "#   'http://www.dianping.com/search/category/4/10/g207r1519p2',\n",
    "#   '粤港烧卤鸭饭(天河店)',\n",
    "#   'http://www.dianping.com/shop/94311152'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf_ls = classify(\"http://www.dianping.com/search/category/4/10\",'广州',\"美食\")\n",
    "print(\"所有二级分类已经获取完毕\")\n",
    "\n",
    "region_ls = list(map(get_region,clf_ls))\n",
    "print(\"获取所有的区域清单\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#解链\n",
    "region_ls = list(itertools.chain.from_iterable(region_ls))\n",
    "region_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#***************获取所有的商圈信息****************************888\n",
    "circle_ls = list(map(get_circle,region_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#解链\n",
    "circle_ls = list(itertools.chain.from_iterable(circle_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def requests_visit_url_test(url,is_use_ip =False,timeout=3):\n",
    "        global ip_pool\n",
    "        global bad_ip\n",
    "        #headers = {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36\"}\n",
    "        headers = {\"Accept\":\"application/json, text/javascript, */*; q=0.01\",\n",
    "                    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "                    \"Accept-Language\":\"zh-CN,zh;q=0.8,en;q=0.6\",\n",
    "                    \"Connection\":\"keep-alive\",\n",
    "                    \"Cookie\":\"_hc.v=f72dbe69-bcfc-a9d4-1f7f-d8daeca1ad7f.1505634421; __utma=1.1801105885.1505634423.1505634423.1505636820.2; __utmz=1.1505634423.1.1.utmcsr=google.com.hk|utmccn=(referral)|utmcmd=referral|utmcct=/; _lxsdk_cuid=15e956d9d5ec8-0a28c942cf39be-2a044871-100200-15e956d9d5fc8; _lxsdk=15e956d9d5ec8-0a28c942cf39be-2a044871-100200-15e956d9d5fc8; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; __mta=222713764.1505836341840.1507819135553.1507819166825.4; JSESSIONID=D2D3D245867334198A59E676F146C5A8; aburl=1; cy=4; cye=guangzhou; s_ViewType=10; _lxsdk_s=15f2012787a-db4-26a-c8%7C%7C47\",\n",
    "                    \"Host\":\"www.dianping.com\",\n",
    "                    \"Referer\":\"http://www.dianping.com/shop/92374637\",\n",
    "                    #\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36\",\n",
    "                    \"X-Requested-With\":\"XMLHttpRequest\"}\n",
    "\n",
    "        #获取任意一个请求头\n",
    "        headers[\"User-Agent\"] = random.choice(GetHeaders().user_agent_list)\n",
    "        bad_ip = []  #用来装坏的ip\n",
    "        \n",
    "        #*********如果不设置ip的话**********************************\n",
    "        if is_use_ip == False:\n",
    "            res = requests.get(url,headers = headers,timeout=timeout)  #访问url，不设代理ip访问\n",
    "            html =  res.content.decode('utf-8')\n",
    "            soup  =  BeautifulSoup(html,'lxml')\n",
    "            if soup.title.text == '403 Forbidden':\n",
    "                print(\"ip被禁止了\")\n",
    "                raise ConnectionError\n",
    "           \n",
    "        #************如果设置ip的话，则执行这一段*********************\n",
    "        else:\n",
    "            is_continue = True   #设置循环标志\n",
    "            while is_continue:\n",
    "            #整理成ip地址的格式\n",
    "                try:\n",
    "                    #\n",
    "                    if len(ip_pool)==len(bad_ip):\n",
    "                        ip_url = ['http://www.xicidaili.com/nn/']\n",
    "                        test_url = \"http://www.dianping.com/shop/2954893\"\n",
    "                        ip_pool= Ip_pool(ip_url).get_ip_pool(test_url)\n",
    "                        prox_ip = random.choice(ip_pool)\n",
    "                        bad_ip = []\n",
    "                        \n",
    "                    #******随机生成一个ip*******\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    http = prox_ip[0]\n",
    "                    ip = prox_ip[1]\n",
    "                    proxies={http:ip}\n",
    "                    print(proxies)\n",
    "                    \n",
    "                    #*******用代理ip访问********\n",
    "                    res = requests.get(url,headers = headers,proxies = proxies,timeout=timeout) #设代理ip访问\n",
    "                    html =  res.content.decode('utf-8')\n",
    "                    soup  =  BeautifulSoup(html,'lxml')\n",
    "            \n",
    "                    #********如果返回网页被禁止的情况，触发异常,否则返回正常**********\n",
    "                    if soup.title.text == '403 Forbidden':\n",
    "                        print(\"ip被禁止了\")\n",
    "                        raise ConnectionError\n",
    "                    else:\n",
    "                        is_continue = False\n",
    "\n",
    "                except HTTPError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"断网\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "\n",
    "\n",
    "                except Timeout as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"超时\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "\n",
    "\n",
    "                except ConnectionError as e:\n",
    "                    print(\"访问被拒\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_cache(circle_ls,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_circle_ls.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circle_ls = read_cache(\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_circle_ls.pickle\")\n",
    "print(len(circle_ls))\n",
    "circle_chunk = get_chunk(circle_ls,5)\n",
    "len(circle_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset=283\n",
    "save_cache(offset,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_offset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    \n",
    "    is_continue = True\n",
    "    while is_continue:\n",
    "\n",
    "        try:\n",
    "            global ip_pool\n",
    "            ip_url = ['http://www.xicidaili.com/nn/']\n",
    "            test_url = \"http://www.dianping.com/shop/2954893\"\n",
    "            ip_pool= Ip_pool(ip_url).get_ip_pool(test_url)\n",
    "\n",
    "            offset = read_cache(\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_offset.pickle\")\n",
    "            end = 660\n",
    "            \n",
    "            #***********多进程处理*************************\n",
    "            for index,chunk in enumerate(circle_chunk[offset:end]):\n",
    "                offset_index = offset + index\n",
    "                print(\"正在爬第{0}chunk\".format(offset_index))\n",
    "                pool = multiprocessing.Pool(processes=3) # 创建3个进程\n",
    "                result = []\n",
    "                for i in chunk:\n",
    "                    result.append(pool.apply_async(get_stores, (i, )))\n",
    "                pool.close() # 关闭进程池，表示不能再往进程池中添加进程，需要在join之前调用\n",
    "                pool.join() # 等待进程池中的所有进程执行完毕\n",
    "                \n",
    "                #***********遍历每个进程的结果*****************\n",
    "                store_ls = []\n",
    "                for res in result:\n",
    "                    store_ls.append(res.get())\n",
    "                \n",
    "                #***********获取每一个chunk的内容****************\n",
    "                print(\"第{0}chunk已经爬完了\".format(offset_index))\n",
    "                store_ls = list(itertools.chain.from_iterable(store_ls))\n",
    "                save_cache(store_ls,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_store_ls_{0}_chunk.pickle\".format(offset_index))\n",
    "                \n",
    "                #********爬完直接将偏移量设为下一个*************************\n",
    "                last_offset_1 = offset_index+1\n",
    "                save_cache(last_offset_1,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_offset.pickle\")\n",
    "                \n",
    "        #***************如果出现ChunkedEncodingError重复执行这段代码*********************     \n",
    "        except ChunkedEncodingError as e:\n",
    "            print(\"出现ChunkedEncodingError\")\n",
    "            last_offset_2 = offset_index-1\n",
    "            print(\"保存上次的offset:{0}\".format(last_offset_2))\n",
    "            save_cache(last_offset_2,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_offset.pickle\")\n",
    "            continue\n",
    "            \n",
    "        #***************如果出现ChunkedEncodingError重复执行这段代码*********************      \n",
    "        except KeyboardInterrupt as e:\n",
    "            print(\"手动停止\")\n",
    "            last_offset_3 = offset_index\n",
    "            print（\"保存现有的offset:{0}\".format(last_offset_3)）\n",
    "            save_cache(last_offset_3,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_offset.pickle\")\n",
    "            break\n",
    "#     print(\"***********准备爬商家的详情信息******************\")\n",
    "#     pool = multiprocessing.Pool(processes=3) # 创建3个进程\n",
    "#     result=[]\n",
    "#     for i in store_ls:\n",
    "#         result.append(pool.apply_async(get_info,(i,)))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "#     print(\"信息页已经爬完了！！\")\n",
    "#     info_ls=[]\n",
    "#     for res in result:\n",
    "#         info_ls.append(res.get())\n",
    "#     save_cache(info_ls,\"/media/zhh/东方国信/智润安排/移动互联网需求/爬虫/缓存/广州_info_test_ls.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
