{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import os.path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from requests.exceptions import ProxyError,ConnectionError,Timeout,HTTPError,ChunkedEncodingError\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import Ipynb_importer\n",
    "from IPpool import GetHeaders\n",
    "from IPpool import Ip_pool\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***************自定义异常类**************************\n",
    "class MyException(Exception):\n",
    "    def __init__(self,message):\n",
    "        Exception.__init__(self)\n",
    "        self.message=message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crawl_common(object):\n",
    "    '''\n",
    "    这是用来爬取大众点评的共有包，包含了以下的函数。\n",
    "    \n",
    "    函数\n",
    "    -------\n",
    "    \n",
    "    1. requests_visit_url(self,url,is_use_ip =False,timeout=3)\n",
    "        函数功能：访问特定的url，带有ip池的功能\n",
    "          \n",
    "    2.  save_cache(self,result,filename):\n",
    "        功能：保留缓存\n",
    "    \n",
    "    3. read_cache(self,filename):\n",
    "        功能：读取数据\n",
    "        \n",
    "    4. get_spc_path(self,father_path,child_path)\n",
    "        功能：切换一个路径或者在路径下新建一个子路径\n",
    "        \n",
    "    5.get_chunk(self,n,adr_ls=None)\n",
    "        功能：将数组分为n组，可以用来分组地去跑函数\n",
    "        \n",
    "    '''\n",
    "    def __init__(self,test_url = 'http://www.dianping.com/shop/95562383',more=False):\n",
    "        '''\n",
    "        功能：生成ip池\n",
    "        \n",
    "        参数\n",
    "        ----\n",
    "        more:bool 类型，more等于True的时候，采取4个网站的ip做ip池\n",
    "        '''\n",
    "        self.test_url = test_url\n",
    "        global ip_pool\n",
    "        global bad_ip\n",
    "        print(\"获取ip池----------------------\")\n",
    "        if more:\n",
    "            print(\"获取4个网站的ip\")\n",
    "            ip_url = ['http://www.xicidaili.com/nn/']\n",
    "            ip_pool= Ip_pool().get_ip_pool(self.test_url,timeout=5)  \n",
    "        else:\n",
    "            print(\"获取1个网站的ip\")\n",
    "            ip_url = ['http://www.xicidaili.com/nn/']\n",
    "            ip_pool= Ip_pool(ip_url).get_ip_pool(self.test_url,timeout=5)  \n",
    "    \n",
    "    #*************带ip池访问url**********************************\n",
    "    def requests_visit_url(self,url,is_use_ip =True,timeout=5):\n",
    "        '''\n",
    "        功能：自带ip池功能来放访问特定的url\n",
    "        \n",
    "        参数\n",
    "        ----\n",
    "        url：sring，表示特定的ul\n",
    "        is_use_ip :bool，True表示采用ip池访问\n",
    "        timeout:int,表示一次访问url的最大超时限制\n",
    "        \n",
    "        结果\n",
    "        ----\n",
    "        soup:BeautifulSoup对象，可以用来解析网页\n",
    "        '''\n",
    "        global ip_pool\n",
    "        global bad_ip\n",
    "        #headers = {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36\"}\n",
    "        headers = {'User-Agent':random.choice(GetHeaders().user_agent_list)}\n",
    "\n",
    "        #获取任意一个请求头\n",
    "        #headers[\"User-Agent\"] = random.choice(GetHeaders().user_agent_list)\n",
    "        bad_ip = []  #用来装坏的ip\n",
    "        \n",
    "        #*********如果不设置ip的话**********************************\n",
    "        if is_use_ip == False:\n",
    "            res = requests.get(url,headers = headers,timeout=timeout)  #访问url，不设代理ip访问\n",
    "            html =  res.content.decode('utf-8')\n",
    "            soup  =  BeautifulSoup(html,'lxml')\n",
    "            if soup.title.text == '403 Forbidden' or \"有道\" in soup.title.text:\n",
    "                print(\"ip被禁止了！！！！！！！！！！！！！！！！\")\n",
    "                raise ConnectionError\n",
    "           \n",
    "        #************如果设置ip的话，则执行这一段*********************\n",
    "        else:\n",
    "            headers = {'User-Agent':random.choice(GetHeaders().user_agent_list)}\n",
    "            is_continue = True   #设置循环标志\n",
    "            while is_continue:\n",
    "            #整理成ip地址的格式\n",
    "                try:\n",
    "                    #\n",
    "                    if len(ip_pool)==len(bad_ip):\n",
    "                        ip_url = ['http://www.xicidaili.com/nn/']\n",
    "                        #self.test_url = 'http://www.dianping.com/shop/95562383'\n",
    "                        ip_pool= Ip_pool().get_ip_pool(self.test_url,timeout=2)\n",
    "                        prox_ip = random.choice(ip_pool)\n",
    "                        bad_ip = []\n",
    "                        \n",
    "                    #******随机生成一个ip*******\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    http = prox_ip[0]\n",
    "                    ip = prox_ip[1]\n",
    "                    proxies={http:ip}\n",
    "                    \n",
    "                    #*******用代理ip访问********\n",
    "                    res = requests.get(url,headers = headers,proxies = proxies,timeout=timeout) #设代理ip访问\n",
    "                    html =  res.content.decode('utf-8')\n",
    "                    soup  =  BeautifulSoup(html,'lxml')\n",
    "                    \n",
    "                    \n",
    "                    #********如果返回网页被禁止的情况，触发异常,否则返回正常**********\n",
    "                    if  soup.title.text == '403 Forbidden' or \"有道\" in soup.title.text:\n",
    "                        print(\"ip被禁止了!!!!!!!!!!!!!!!!!!!!\")\n",
    "                        raise ConnectionError\n",
    "                    else:\n",
    "                        is_continue = False\n",
    "\n",
    "                except HTTPError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"断网\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "\n",
    "                except Timeout as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"超时\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "\n",
    "                except ConnectionError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"访问被拒\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except AttributeError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"----------\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except ChunkedEncodingError as e:\n",
    "                    bad_ip.append(prox_ip)\n",
    "                    print(\"ChunkedEncodingError\")\n",
    "                    prox_ip = random.choice(ip_pool)\n",
    "                    time.sleep(2)\n",
    "\n",
    "        return soup\n",
    "    \n",
    "\n",
    "    #**************缓存结果******************\n",
    "    def save_cache(self,result,filename):\n",
    "        '''\n",
    "        功能：保留缓存\n",
    "        \n",
    "        参数\n",
    "        -------\n",
    "        result:任何类型，保存的结果。\n",
    "        filename:字符串，保存的路径。\n",
    "        \n",
    "        返回值\n",
    "        -------\n",
    "        无\n",
    "        '''\n",
    "        with open(filename,'wb') as f1:\n",
    "            pickle.dump(result,f1)\n",
    "            \n",
    "    #---------加载缓存-----------------------\n",
    "    def read_cache(self,filename):\n",
    "        '''\n",
    "        功能：读取数据\n",
    "        \n",
    "        参数\n",
    "        ------\n",
    "        filename:字符串，文件的路径\n",
    "        \n",
    "        返回值：\n",
    "        ----\n",
    "        result：任何对象\n",
    "        '''\n",
    "        with open(filename,'rb')as f1:\n",
    "            result = pickle.load(f1)\n",
    "        return result\n",
    "    \n",
    "    #----------切换路径-------------------\n",
    "    def get_spc_path(self,father_path,child_path):\n",
    "        '''\n",
    "        功能：切换路径，创建文件夹\n",
    "        \n",
    "        参数\n",
    "        -----\n",
    "        father_path:字符串，绝对路径的父目录\n",
    "        child_path：字符串，绝对路径的子目录\n",
    "        '''\n",
    "        \n",
    "        os.chdir(father_path)\n",
    "        if os.path.exists(child_path):\n",
    "            os.chdir(child_path)\n",
    "        else:\n",
    "            os.mkdir(child_path)\n",
    "            os.chdir(child_path)\n",
    "        print(\"done!!\")\n",
    "\n",
    "    def get_chunk(self,n,adr_ls=None):\n",
    "        '''\n",
    "        功能：将数组分为n组，可以用来分组地去跑函数\n",
    "        \n",
    "        参数：\n",
    "        ----\n",
    "        n:int型，表示将数组分为n组\n",
    "        adr_ls:list，表示要被切分的数组\n",
    "        '''\n",
    "        new_adr_ls = copy.deepcopy(adr_ls)\n",
    "        adr_chunk = [new_adr_ls[i:i + n] for i in range(0, len(new_adr_ls), n)]\n",
    "        return adr_chunk\n",
    "        \n",
    "        \n",
    "#     #---------定位一级分类，爬取二级分类------------------\n",
    "#     def classify(self,city_url,city,big_class):\n",
    "#         soup = requests_visit_url(city_url)\n",
    "#         clf_ls = []\n",
    "#         for i in soup.select(\"#classfy a\"):\n",
    "#             clf_url = i[\"href\"] #分类网址\n",
    "#             clf_tl = i.text.strip() # 分类标题\n",
    "#             clf_ls.append([big_class,city,clf_tl,clf_url])\n",
    "#         return clf_ls\n",
    "    \n",
    "#     #定位二级分类，爬取区县（行政区）\n",
    "#     def get_region(self,class_ls=None):\n",
    "#         url = class_ls[-1]  #url\n",
    "#         soup = requests_visit_url(url) #访问\n",
    "#         region_ls = [] #地区的列表\n",
    "#         for i in soup.select(\"#region-nav a\"):\n",
    "#             if i.text.strip() != \"不限\":\n",
    "#                 region_url = i[\"href\"] \n",
    "#                 region_tl = i.text.strip()\n",
    "#                 region = class_ls[:-1]\n",
    "#                 region.extend([region_tl,region_url])\n",
    "#                 region_ls.append(region)\n",
    "#         return region_ls\n",
    "    \n",
    "#     #-----------定位了区县，爬取商圈-----------\n",
    "#     def get_circle(self,region_ls=None):\n",
    "#         url = region_ls[-1]   #url网址\n",
    "#         soup = self.requests_visit_url(url) #访问BeautifulSoup\n",
    "#         circle_ls =[]             \n",
    "#         for i in soup.select(\"#region-nav-sub a\"):  #商圈信息\n",
    "#             if i.span.text.strip()!=\"不限\":   #剔除掉不限的url\n",
    "#                 circle_url = i[\"href\"]       #商圈url\n",
    "#                 circle_tit = i.span.text.strip()  #商圈名\n",
    "#                 circle = region_ls[:-1] \n",
    "#                 circle.extend([circle_tit,circle_url]) \n",
    "#                 circle_ls.append(circle)\n",
    "\n",
    "#         #如果没有商圈的话，就拿原来的url当作是商圈的url\n",
    "#         if circle_ls==[]:\n",
    "#             print(region_ls[-2]+\"没有子分类！！\")\n",
    "#             region_ls.insert(4,\"没有商圈\")\n",
    "#             circle_ls.append(region_ls)\n",
    "\n",
    "#         return circle_ls\n",
    "    \n",
    "#     #---------------获取商家具体信息----------------------\n",
    "#     #获取详情层的数据，如星级用户、浏览人数、均价\n",
    "#     def get_info(store_ls=None,prox_ip=None):\n",
    "\n",
    "#         #首先先保证导入的数据不为空\n",
    "#         if store_ls !=[]:\n",
    "#             url = store_ls[-1] #详情信息表\n",
    "#             soup = requests_visit_url(url,prox_ip)\n",
    "#             print(soup)\n",
    "#             try:\n",
    "#                 start = soup.select(\".mid-rank-stars\")[0][\"title\"]       #星级用户\n",
    "#             except Exception as e:\n",
    "#                 print(\"start\",e)\n",
    "#                 start = ''\n",
    "\n",
    "#             try:\n",
    "#                 review_count = soup.select(\"#reviewCount\")[0].text  #浏览人数\n",
    "#             except Exception as e:\n",
    "#                 review_count = ''\n",
    "#                 print(\"review_coun\",e)\n",
    "\n",
    "#             try:\n",
    "#                 avgprice = soup.select(\"#avgPriceTitle\")[0].text   #均价\n",
    "#             except Exception as e:\n",
    "#                 avgprice = ''\n",
    "#                 print(\"avgprice\",e)\n",
    "\n",
    "#             #评分系统\n",
    "#             try:\n",
    "#                 score= []\n",
    "#                 for j in soup.select(\"#comment_score .item\"):\n",
    "#                     score.append(j.text)\n",
    "#                 score = \"|\".join(score)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 score = \"\"\n",
    "#                 print(\"score\",e)\n",
    "\n",
    "#             try:   \n",
    "#                 address = soup.select(\".address .item\")[0].text.strip() #地址\n",
    "#             except Exception as e:\n",
    "#                 address = ''\n",
    "#                 print(\"address出错了\",e)\n",
    "\n",
    "#             #电话号码\n",
    "#             try:\n",
    "#                 tel =[]\n",
    "#                 for i in soup.select(\".tel .item\"):\n",
    "#                     tel.append(i.text)\n",
    "#                 tel = \"|\".join(tel)\n",
    "\n",
    "#             except:\n",
    "#                 tel=\"\"\n",
    "#                 print(\"tel出错了\",e)\n",
    "\n",
    "#             #路径\n",
    "#             try:\n",
    "#                 path = []\n",
    "#                 for i in soup.select(\".breadcrumb a\"):\n",
    "#                     path.append(i.text.strip())\n",
    "#                 path ='>'.join(path)\n",
    "#             except Exception as e:\n",
    "#                 print(\"path出错了\",e)\n",
    "#                 path =''\n",
    "\n",
    "#             info = store_ls[:]\n",
    "#             info.extend([start,review_count,avgprice,score,address,tel,path])\n",
    "#         return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
